{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PutyznCfnH1c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "import bs4\n",
    "import re\n",
    "import tqdm\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "\n",
    "# RNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "pd.set_option(\"max.columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Gtq5FIZnH1f"
   },
   "source": [
    "This project is an attempt to predict a Pitchfork review rating (ranging from 0 to 10 with steps of 0.1) using an RNN trained on the review's content (with Tensorflow/Keras). For that, I initially collected all reviews from Pitchfork's website, cleaned the data, used GloVe embeddings with 50 dimensions to transform words into vectors, and finally trained several models, choosing the one with the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIvmNkHHnH1f"
   },
   "source": [
    "#### 1) Get data from all reviews available on Pitchfork's website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCBGZgyUnH1f"
   },
   "outputs": [],
   "source": [
    "# Define headers for API request\n",
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Kanguage': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Host': 'pitchfork.com',\n",
    "    'Referer': 'https://pitchfork.com/reviews/albums/',\n",
    "    'Sec-Fetch-Dest': 'empty',\n",
    "    'Sec-Fetch-Mode': 'cors',\n",
    "    'Sec-Fetch-Site': 'same-origin',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m46IQBmWnH1f"
   },
   "outputs": [],
   "source": [
    "# Function to get JSON review data\n",
    "def get_reviews_json(url, start=0):\n",
    "    url_search = url.format(size=size, start=start)\n",
    "    return rq.get(url_search, headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1YCTNKhnH1f",
    "outputId": "4e20ab18-1324-4949-c942-2367783bbe04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [13:13<00:00,  6.90s/it]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://pitchfork.com/api/v2/search/?types=reviews&hierarchy=sections%2Freviews%2Falbums%2Cchannels%2Freviews%2Falbums&sort=publishdate%20desc%2Cposition%20asc&size={size}&start={start}'\n",
    "size = 200\n",
    "\n",
    "# Get total_count\n",
    "reviews_json = get_reviews_json(url)\n",
    "total_count = pd.json_normalize(reviews_json)['count'][0]\n",
    "reviews_raw_df = pd.json_normalize(pd.json_normalize(reviews_json)['results.list'][0])\n",
    "\n",
    "for i in tqdm.tqdm(range(200, total_count, 200)):\n",
    "    reviews_json = get_reviews_json(url, start=i)\n",
    "    time.sleep(2)\n",
    "    aux_df = pd.json_normalize(pd.json_normalize(reviews_json)['results.list'][0])\n",
    "    reviews_raw_df = pd.concat([reviews_raw_df, aux_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA-iqUganH1g",
    "outputId": "26706018-8aea-4c02-d925-4bb0185fce7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23091, 25)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_raw_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbAR1G41nH1g"
   },
   "source": [
    "Info collected for each review:\n",
    "- URL\n",
    "- Artists\n",
    "- Album\n",
    "- Music Label\n",
    "- Genres\n",
    "- Author\n",
    "- Review\n",
    "- Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42dLk3lmnH1g"
   },
   "outputs": [],
   "source": [
    "# Assemble and process relevant df\n",
    "reviews_df = pd.DataFrame()\n",
    "reviews_df['url'] = reviews_raw_df['url'].copy()\n",
    "reviews_df['artists'] = ''\n",
    "reviews_df['album'] = reviews_raw_df['seoTitle'].copy()\n",
    "reviews_df['label'] = ''\n",
    "reviews_df['genres'] = ''\n",
    "reviews_df['author'] = ''\n",
    "reviews_df['review'] = ''\n",
    "reviews_df['rating'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA3Q4Dy7nH1g"
   },
   "outputs": [],
   "source": [
    "# Function to access, save and return each review's content\n",
    "def get_and_save_review_content(num, save=False):\n",
    "    url_review = 'https://pitchfork.com'+reviews_df.iloc[num,0]\n",
    "    resp = rq.get(url_review)\n",
    "    \n",
    "    # Save HTML\n",
    "    if save:\n",
    "        name = reviews_df.iloc[num,0][16:-1]\n",
    "        with open(\"./Reviews/{}.html\".format(name), 'w+', encoding='utf-8') as output:\n",
    "            output.write(resp.text)\n",
    "    \n",
    "    # Retrieve content\n",
    "    parsed_html = bs4.BeautifulSoup(resp.text)\n",
    "    description_test = parsed_html.find('div', attrs={'class': 'review-detail__abstract'})\n",
    "    content_test = parsed_html.find('div', attrs={'class': re.compile(r\"contents\")})\n",
    "    \n",
    "    # If first attempt to access page fails, try again until successful\n",
    "    while (description_test == None) | (content_test == None):\n",
    "        time.sleep(5)\n",
    "        resp = rq.get(url_review)\n",
    "        parsed_html = bs4.BeautifulSoup(resp.text)\n",
    "        description_test = parsed_html.find('div', attrs={'class': 'review-detail__abstract'})\n",
    "        content_test = parsed_html.find('div', attrs={'class': re.compile(r\"contents\")})\n",
    "        \n",
    "    description = parsed_html.find('div', attrs={'class': 'review-detail__abstract'}).get_text().strip()\n",
    "    content_raw = parsed_html.find('div', attrs={'class': re.compile(r\"contents\")}).get_text().strip()\n",
    "\n",
    "    # Remove irrelevant text\n",
    "    if re.search('\\n\\n', content_raw) != None:\n",
    "        content_limit = re.search('\\n\\n', content_raw).span()[0]\n",
    "        content = content_raw[:content_limit]\n",
    "    else:\n",
    "        content = content_raw\n",
    "    \n",
    "    review = description + \" \" + content\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9AdCs2tnH1g"
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "for i in tqdm.tqdm(range(start,total_count)):\n",
    "    \n",
    "    aux_df = pd.json_normalize(reviews_raw_df.iloc[i,24])\n",
    "    \n",
    "    # Artists\n",
    "    if reviews_raw_df.iloc[i,0] != []:\n",
    "        artists = pd.json_normalize(reviews_raw_df.iloc[i,0])['display_name'].str.cat(sep=' ')\n",
    "        reviews_df['artists'][i] = artists\n",
    "    \n",
    "    # Label\n",
    "    if aux_df.empty == False:\n",
    "        if pd.json_normalize(aux_df['labels_and_years'][0])['labels'][0] != []:\n",
    "            label = pd.json_normalize(pd.json_normalize(aux_df['labels_and_years'][0])['labels'][0])['name'].str.cat(sep=' ')\n",
    "            reviews_df['label'][i] = label\n",
    "    \n",
    "    # Genres\n",
    "    if reviews_raw_df.iloc[i,1] != []:\n",
    "        genres = pd.json_normalize(reviews_raw_df.iloc[i,1])['slug'].str.cat(sep=' ')\n",
    "        reviews_df['genres'][i] = genres\n",
    "    \n",
    "    # Author\n",
    "    author = pd.json_normalize(reviews_raw_df.iloc[i,12])['name'].str.cat(sep=' ')\n",
    "    reviews_df['author'][i] = author\n",
    "    \n",
    "    # Review\n",
    "    review = get_and_save_review_content(num=i)\n",
    "    reviews_df['review'][i] = review\n",
    "    \n",
    "    # Rating\n",
    "    if aux_df.empty == False:\n",
    "        rating = aux_df['rating.rating'][0]\n",
    "        reviews_df['rating'][i] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eefU_oX8nH1g"
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "reviews_df = reviews_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Replace blank values with NaN and drop rows with NaNs on either review or rating columns\n",
    "reviews_df = reviews_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "reviews_df.dropna(subset=['review', 'rating'], how='any', axis=0, inplace=True)\n",
    "reviews_df = reviews_df.reset_index(drop=True)\n",
    "\n",
    "# Save relevant df\n",
    "reviews_df.to_csv('./reviews_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQVkQ44cnH1g",
    "outputId": "f0d233c7-b3c8-44f7-8f43-cee632613974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 23088 entries, 0 to 23090\n",
      "Data columns (total 8 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   url      23088 non-null  object\n",
      " 1   artists  22311 non-null  object\n",
      " 2   album    23079 non-null  object\n",
      " 3   label    23050 non-null  object\n",
      " 4   genres   20757 non-null  object\n",
      " 5   author   23088 non-null  object\n",
      " 6   review   23088 non-null  object\n",
      " 7   rating   23088 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrUDalPUnH1g"
   },
   "source": [
    "#### 2) Clean review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3gpOuSOnH1g"
   },
   "outputs": [],
   "source": [
    "# Function to remove extra whitespace\n",
    "def remove_whitespace(series):\n",
    "    series = series.str.replace(\"\\n\",\" \")\n",
    "    series = series.str.replace(\" +\",\" \")\n",
    "    series = series.str.strip()\n",
    "    return series\n",
    "\n",
    "# Function to remove uppercase and specific punctuation\n",
    "def normalize(series):\n",
    "    series = series.str.lower()\n",
    "    series = series.str.replace(r\"\\xa0|\\\\xbd|\\\\|\\/|\\\"|\\“|\\”|\\-|\\,|\\—|\\;|\\:|\\.|\\?|\\!|\\(|\\)|\\_|\\*\",\" \")\n",
    "    series = series.str.strip()\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdkNx0X6nH1h"
   },
   "outputs": [],
   "source": [
    "reviews_df['review'] = remove_whitespace(normalize(remove_whitespace(reviews_df['review'])))\n",
    "\n",
    "# Save clean df version\n",
    "reviews_df.to_csv('./reviews_clean_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAIa-CT1nH1h"
   },
   "source": [
    "#### 3) Load data (whenever needed) and explore it a little\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "K0MF2jwqnH1h"
   },
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('./reviews_clean_df.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "X = reviews_df['review'].astype(str).copy()\n",
    "#Y = reviews_df['rating'].copy()/10                                  # Used for sigmoid activation output (regression RNN)\n",
    "Y_explore = (reviews_df['rating'].copy()).astype(int)\n",
    "Y = np.asarray(Y_explore)                                            # Used for softmax activation output (multi-class classification RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBCABLXJVslh",
    "outputId": "0d1dc85c-ad02-4eff-ed70-0fd5801bb908"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     42.370165\n",
       "6     22.224629\n",
       "8     17.706935\n",
       "5      8.558929\n",
       "4      3.378525\n",
       "9      2.585871\n",
       "3      1.593971\n",
       "2      0.762334\n",
       "10     0.498116\n",
       "1      0.203578\n",
       "0      0.116949\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_explore.value_counts()*100/len(Y_explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C98hbiJcemL"
   },
   "source": [
    "Very few observations with low ratings. That means the RNN possibly won't be able to learn the particularities of these type of reviews and won't predict much on this lower range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YStVV_uWnH1h"
   },
   "source": [
    "#### 4) Word embedding with GloVe 50D (with review padding whenever necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y8H_3zfHnH1h"
   },
   "outputs": [],
   "source": [
    "# Function to read GloVe file and define useful dictionaries\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('./glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmdlIhFfnH1h"
   },
   "source": [
    "Where:  \n",
    "- word_to_index: dictionary mapping from words to their indices in the vocabulary \n",
    "- index_to_word: dictionary mapping from indices to their corresponding words in the vocabulary\n",
    "- word_to_vec_map: dictionary mapping words to their GloVe vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vyzQBD8XnH1h"
   },
   "outputs": [],
   "source": [
    "# Function to set maximum review length. If max_possible=True, use number of words of the longest review\n",
    "def set_max_review_length(n, max_possible=False):\n",
    "    if max_possible:\n",
    "        return len(max(X, key=len).split())\n",
    "    else:\n",
    "        return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plT4sZlInH1h"
   },
   "source": [
    "For different models, I used different maximum review lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WK6B1jRunH1h"
   },
   "outputs": [],
   "source": [
    "# Function that processes an array of sentences (X) and returns inputs (lists of indices) to the embedding layer\n",
    "def sentences_to_indices(X, word_to_index, max_review_length, begin_end=False):\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    X_indices = np.zeros((m, max_review_length))\n",
    "    \n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].split()\n",
    "        if begin_end:\n",
    "            half = int(max_review_length/2)\n",
    "            sentence_words = sentence_words[:half] + sentence_words[-half:]\n",
    "        \n",
    "        j = 0\n",
    "        for w in sentence_words[:max_review_length]:\n",
    "            X_indices[i,j] = word_to_index.get(w, word_to_index['<UNK>'])\n",
    "            j += 1\n",
    "  \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOr8LiGRnH1h",
    "outputId": "bb65dc19-1222-4d75-dd58-3e7d2adefe37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[357267., 148909., 223945., ...,      0.,      0.,      0.],\n",
       "       [357267.,  17044., 393303., ...,      0.,      0.,      0.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test of function above\n",
    "X1 = np.array([X[0], X[1]])\n",
    "X1_indices = sentences_to_indices(X1, word_to_index, max_review_length)\n",
    "X1_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL4tG2PinH1h"
   },
   "source": [
    "#### 5) Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvg7t3QanH1h"
   },
   "source": [
    "Split data into train, dev and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAtQHfbpnH1h",
    "outputId": "109b4420-89dd-48fd-bc39-1d6975c0c2c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 1000 1087\n"
     ]
    }
   ],
   "source": [
    "# Define maximum review length and size of each split\n",
    "max_review_length = set_max_review_length(1500, False)\n",
    "X_indices = sentences_to_indices(X, word_to_index, max_review_length, begin_end=False)\n",
    "\n",
    "num_train = 21000\n",
    "num_dev = 1000\n",
    "num_test = len(X) - num_train - num_dev\n",
    "\n",
    "X_train, Y_train = X[:num_train].copy(), Y[:num_train].copy()\n",
    "X_dev, Y_dev = X[num_train:num_train+num_dev].copy(), Y[num_train:num_train+num_dev].copy()\n",
    "X_test, Y_test = X[num_train+num_dev:].copy(), Y[num_train+num_dev:].copy()\n",
    "\n",
    "X_indices_train = X_indices[:num_train].copy()\n",
    "X_indices_dev = X_indices[num_train:num_train+num_dev].copy()\n",
    "X_indices_test = X_indices[num_train+num_dev:].copy()\n",
    "\n",
    "print(len(X_train), len(X_dev), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCGyUnCRnH1h"
   },
   "source": [
    "Create embedding layer (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ijk2thRnH1h"
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):   \n",
    "    # Adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    embedding_dim = word_to_vec_map[\"the\"].shape[0]  \n",
    "    emb_matrix = np.zeros((vocab_len, embedding_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx,:] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=embedding_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlvHnvVnnH1h"
   },
   "source": [
    "Convert labels to one-hot vectors (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0wk_QIHnnH1h"
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    return np.eye(C)[Y.reshape(-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJPSN-V6nH1h"
   },
   "source": [
    "Build model (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPo_mri5nH1h"
   },
   "outputs": [],
   "source": [
    "def review_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    # Input\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    # Embedding\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    # LSTM\n",
    "    X = LSTM(units=128, return_sequences=True)(embeddings)\n",
    "    # Dropout, rate is the probability of zeroing nodes\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    # LSTM 2\n",
    "    X = LSTM(units=128, return_sequences=True)(X)\n",
    "    # Dropout 2\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    # LSTM 3\n",
    "    X = LSTM(units=128, return_sequences=False)(X)\n",
    "    # Dropout 3\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    # Dense\n",
    "    X = Dense(units=11)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    review_model = Model(inputs=sentence_indices,outputs=X)\n",
    "\n",
    "    return review_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd3C1gZznH1h"
   },
   "source": [
    "Set hyperparameters and specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaWZyo7jnH1h"
   },
   "outputs": [],
   "source": [
    "# Learning rate decay schedule\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "batch_size = 128\n",
    "opt = Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "loss = 'categorical_crossentropy'\n",
    "checkpoint = ModelCheckpoint(\"./best_model_4.hdf5\", monitor='accuracy', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "def specify_model(model, opt, loss):\n",
    "    model = model((max_review_length,), word_to_vec_map, word_to_index)\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ak77vZAnH1h"
   },
   "source": [
    "Convert labels to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uPhnD1fonH1h"
   },
   "outputs": [],
   "source": [
    "Y_train_oh = convert_to_one_hot(Y_train, C=11)\n",
    "Y_dev_oh = convert_to_one_hot(Y_dev, C=11)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDW6PBtonH1h"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYwnZwF6nH1h"
   },
   "outputs": [],
   "source": [
    "review_model = specify_model(review_model, opt, loss)\n",
    "review_model.fit(X_indices_train, Y_train_oh, epochs=100, batch_size=batch_size, shuffle=True, verbose=1, \n",
    "                 callbacks=[checkpoint], validation_data=(X_indices_dev, Y_dev_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FWlNjCjnH1h"
   },
   "source": [
    "Evaluate model (on train, dev or test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KKJ0m6oInH1h"
   },
   "outputs": [],
   "source": [
    "# Load different model (if needed)\n",
    "review_model = load_model('./best_model_3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aGQMC5nbnH1h"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_indices, Y, Y_oh):\n",
    "    pred = model.predict(X_indices)\n",
    "    \n",
    "    pred_labels = []\n",
    "    for p in pred:\n",
    "        pred_labels.append(np.argmax(p))\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    \n",
    "    max_pred_rating = max(pred_labels)\n",
    "    min_pred_rating = min(pred_labels)\n",
    "    diff = np.absolute(Y-pred_labels)\n",
    "    max_diff = np.max(diff)\n",
    "    mse = np.sum((diff)**2)/num_dev\n",
    "    rmse = np.sqrt(mse)\n",
    "    loss, acc = review_model.evaluate(X_indices, Y_oh)\n",
    "    print(\"MSE: {}\\nRMSE: {}\\nMax predicted rating: {}\\nMin predicted rating: {}\\nMax difference of ratings: {}\".format(mse, rmse, max_pred_rating, min_pred_rating, max_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-stzh1HCnH1h",
    "outputId": "258f1c23-4a53-413b-a4ae-70d261bff6e0"
   },
   "outputs": [],
   "source": [
    "# On train data\n",
    "evaluate_model(review_model, X_indices_train, Y_train, Y_train_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpdSPKihnH1h",
    "outputId": "d3eed245-0400-4106-d6e7-7423ed66399e"
   },
   "outputs": [],
   "source": [
    "# On dev data\n",
    "evaluate_model(review_model, X_indices_dev, Y_dev, Y_dev_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Et6I5X77nH1h",
    "outputId": "b7bf475a-3302-4503-be1a-2d320f531d30"
   },
   "outputs": [],
   "source": [
    "# On test data\n",
    "evaluate_model(review_model, X_indices_test, Y_test, Y_test_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQtOxNSRnH1h"
   },
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Usskx1nInH1h"
   },
   "outputs": [],
   "source": [
    "review_model.save(\"./review_model.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pitchfork_vgit.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
